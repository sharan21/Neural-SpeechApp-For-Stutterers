{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf100
{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\fswiss\fcharset0 Helvetica-Oblique;
\f3\fswiss\fcharset0 Helvetica-BoldOblique;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 ABSTRACT:\

\f1\b0 \
With the techniques developed for habitual disorders such as speech disorders, it would be useful to have a device with you to constantly monitor your behavior and give signals to indicate if the proper technique is being used or not.\
\
In the case of stammering, such a device can be used to assist the speaker, by processing each word spoken, analysing the features of the speech data, and outputing a signal or feed back to tell the user if the proper technique is being used or not.\
\
Techniques such as Long Lengthening speaking, a technique developed by speech pathologist Partha Bagchi, is a well known technique and
\f0\b  when used properly
\f1\b0 , can enable a stammerer to speak fluently in 
\f2\i most
\f1\i0  situations. More about the technique and how it works mentioned later. Given that the proper practices and guidelines are being employed by the speaker, the speech is nearly fluent, with minor blockages.\
\
 Furthermore, after prolonged practice using this device, the technique becomes habituated/involuntary, and substitutes the faultly speech mechanism with the fluent one, at which point the device is not needed anymore.\
\

\f3\i\b \ul Why do we need a device if the technqiue already guarentees fluency?
\f1\i0\b0 \ulnone \
\
The only issue with employing such practice in real situations, is that:\
\
1. people often forgot to use it,\
2. are too embarressed to speak differently\
3. or are distracted due to stress or over consciousness.\
\
Furthermore is stressfull conditions, it becomes hard to focus on the technique, and users unknowingly end up using the wrong/partial technique without knowing.\
\
Such a device will ensure that every word is spoken perfectly, and will constantly remind the user to be conscious of it. After continous practice, the speed of talking can be increased and the device is no longer needed.\
\

\f0\b TECHNIQUE:\
\

\f1\b0 \
\
\

\f0\b EXPECTED RESULTS:
\f1\b0 \
\
To have a trained deep learning model, which takes a each word spoken in real time (Wav -> numpy array), and outputs 0/1 (0 for wrong use, 1 for proper use), constantly giving feedback to the user.\
\
To test this device with different people and finetune the hyper parameters. If the neural network does not over fit the data, and employes regularization, and has been trained with a data set of a large enough size, it will most work with multiple individuals, regardless of gender/ frequency of speech.\
\
\

\f0\b METHODS/ PLANS: (in Python environment)
\f1\b0 \
\
1. Preparing the training data set, cleaning it for training. \
\
As mentioned earlier, the input to the tf graph consists of a numpy array of values, each array corresponds to a word in the sentence. \
\
Therefore to prepare the data we must convert \ul an audio sentence in wav format \'97> numpy list of data , \ulnone where number of rows = 1000 (number of nodes in the input layer) or number of samples in the audio word data.\
\

\f0\b DEPENDANCIES:\
\

\f1\b0 numpy\
scipy\
matplotlib\
pydub\
os\
wave \
pyaudio\ul \
\ulnone \

\f0\b \ul Steps to prepare chunks of numpy \'93word data\'94: 
\f2\i\b0 \ulnone (All necessary python files and directories are found in project folder)\

\f1\i0 \ul \
\ulnone 	1a. 
\f0\b get_word.py
\f1\b0 : starts recording and stores sentences in wav format in \'93output.wav\'94\
	1b. 
\f0\b get_words.py
\f1\b0 : takes \'93output.wav\'94 and splits it into audio chunks corresponding to each word and stores in the folder all_chunks.\
	1c. 
\f0\b import_words.py
\f1\b0 : takes all the audio wav chunks in \'93all_chunks\'94 directory and converts them into numpy array, for further processing.\
	1d. 
\f0\b import_words.py:
\f1\b0  contains func convertToMp3() to convert each wav chunk to mp3 and store in mp3_chunks. Also contains plotAll() to plot each numpy audio chunk.\
\
	\

\f0\b \ul Cleaning/processing of numpy audio chunk data. \
\
	
\f1\b0 \ulnone 1e. 
\f0\b clean_data.py:
\f1\b0  soundData contains numpy list of values for all words, and is roughly 22000 samples in size. To remove redundant data, reduceDensity() is used to reduce it to roughly 1000 samples in size.\
	1f. 
\f0\b clean_data.py:
\f1\b0  trimmer() is use to cut the start and end edges of the numpy word chunk, to make sure all chunks are of equal sample size.\
	1g. 
\f0\b clean_data.py:
\f1\b0  weightedAverage() is used to extrapolate the noisy oscillating sound data for easier processing for tf model. (or smoothen the data)\
	1h. 
\f0\b envelope_data.py:
\f1\b0  (Ongoing) uses RC filter to further smoothen the data.\
\
\
2. Preparing the Tensorflow model (Ongoing)\
	2a. 
\f0\b Input layer:
\f1\b0  1000 nodes\
	2b. 
\f0\b Hidden layers:
\f1\b0  3 hidden layers  
\f2\i (with batch norm and dropout during training)\
	
\f1\i0 2c. 
\f0\b Output layer:
\f1\b0  1 node (0/1) 
\f2\i (hardmax classifier to round off to 0 or 1)
\f1\i0 \
	\
\
\
3. Tuning the Hyper parameters of model to improve accuracy of dev/test seg\
\
\
4. Developing App for real life use.\
\
	4a. In Android, and iOS\
\
\
5. Testing app and fine tuning.\
\
\
6. Publishing and Documentation.\
\
\
\
\
\
\
\
\
\
\
}